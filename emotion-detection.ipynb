{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-11-27T15:24:32.611423Z","iopub.execute_input":"2021-11-27T15:24:32.612069Z","iopub.status.idle":"2021-11-27T15:24:32.654836Z","shell.execute_reply.started":"2021-11-27T15:24:32.611934Z","shell.execute_reply":"2021-11-27T15:24:32.653753Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"import library","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport math\nimport mlxtend\nimport sklearn.cluster as cluster\nimport sklearn.neighbors\nimport sklearn.metrics as metrics\nimport re\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport string\nfrom sklearn.metrics import ConfusionMatrixDisplay\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import classification_report,confusion_matrix, roc_curve, auc, precision_score, recall_score, f1_score, precision_recall_curve\nimport nltk\nnltk.download('wordnet')\nnltk.download('stopwords')\nfrom nltk.tokenize import word_tokenize \nfrom nltk.tokenize import RegexpTokenizer\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2021-11-27T15:24:38.214954Z","iopub.execute_input":"2021-11-27T15:24:38.215257Z","iopub.status.idle":"2021-11-27T15:25:19.973143Z","shell.execute_reply.started":"2021-11-27T15:24:38.215222Z","shell.execute_reply":"2021-11-27T15:25:19.972402Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"train_data = pd.read_csv('../input/emotions-dataset-for-nlp/train.txt',names=['sentence','emotion'],header=None, sep=';')\ntest_data = pd.read_csv('../input/emotions-dataset-for-nlp/test.txt',names=['sentence','emotion'],header=None, sep=';')\nval_data= pd.read_csv('../input/emotions-dataset-for-nlp/val.txt',names=['sentence','emotion'],header=None, sep=';')\ndata = pd.concat([train_data,test_data, val_data])\nprint('Total data:',data.shape)\n","metadata":{"execution":{"iopub.status.busy":"2021-11-27T15:32:20.764773Z","iopub.execute_input":"2021-11-27T15:32:20.765180Z","iopub.status.idle":"2021-11-27T15:32:20.832124Z","shell.execute_reply.started":"2021-11-27T15:32:20.765144Z","shell.execute_reply":"2021-11-27T15:32:20.831166Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"train_data.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-27T15:32:25.795696Z","iopub.execute_input":"2021-11-27T15:32:25.796359Z","iopub.status.idle":"2021-11-27T15:32:25.807176Z","shell.execute_reply.started":"2021-11-27T15:32:25.796310Z","shell.execute_reply":"2021-11-27T15:32:25.806445Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"data.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-11-27T15:32:30.007875Z","iopub.execute_input":"2021-11-27T15:32:30.008798Z","iopub.status.idle":"2021-11-27T15:32:30.020682Z","shell.execute_reply.started":"2021-11-27T15:32:30.008757Z","shell.execute_reply":"2021-11-27T15:32:30.019802Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"data.emotion.unique()","metadata":{"execution":{"iopub.status.busy":"2021-11-27T15:32:32.659433Z","iopub.execute_input":"2021-11-27T15:32:32.659694Z","iopub.status.idle":"2021-11-27T15:32:32.668778Z","shell.execute_reply.started":"2021-11-27T15:32:32.659667Z","shell.execute_reply":"2021-11-27T15:32:32.667998Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"data['emotion'].value_counts(normalize=True)\n","metadata":{"execution":{"iopub.status.busy":"2021-11-27T15:32:34.048635Z","iopub.execute_input":"2021-11-27T15:32:34.049595Z","iopub.status.idle":"2021-11-27T15:32:34.066463Z","shell.execute_reply.started":"2021-11-27T15:32:34.049494Z","shell.execute_reply":"2021-11-27T15:32:34.064731Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"data.info","metadata":{"execution":{"iopub.status.busy":"2021-11-27T15:32:36.892443Z","iopub.execute_input":"2021-11-27T15:32:36.893019Z","iopub.status.idle":"2021-11-27T15:32:36.902318Z","shell.execute_reply.started":"2021-11-27T15:32:36.892971Z","shell.execute_reply":"2021-11-27T15:32:36.901268Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"#for convert words to numerical data by mapping \n#emotion_mapping={'sadness':1,'anger':2,'love':3,'surprise':4,\"fear\":5,\"joy\":6}\n\n#data['emotion']=data['emotion'].map(emotion_mapping)\n#data.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-27T15:32:38.313009Z","iopub.execute_input":"2021-11-27T15:32:38.313710Z","iopub.status.idle":"2021-11-27T15:32:38.316710Z","shell.execute_reply.started":"2021-11-27T15:32:38.313677Z","shell.execute_reply":"2021-11-27T15:32:38.315991Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"data['length'] = data['sentence'].apply(len) # to add length column \ndata.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-27T15:32:40.993656Z","iopub.execute_input":"2021-11-27T15:32:40.994384Z","iopub.status.idle":"2021-11-27T15:32:41.018703Z","shell.execute_reply.started":"2021-11-27T15:32:40.994343Z","shell.execute_reply":"2021-11-27T15:32:41.017883Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"# convert the emotions to binary labels. love and joy emotions are \"not-stressed ==1\", and sadness, anger, fear, and surprise are \"stressed == 0\".\ndata['label']=data['emotion'].replace({'joy': \"not-stressed\" , 'love': \"not-stressed\", \n                                   'sadness': \"stressed\", 'anger': \"stressed\", 'fear': \"stressed\",'surprise': \"stressed\"})\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-27T15:32:43.823811Z","iopub.execute_input":"2021-11-27T15:32:43.824112Z","iopub.status.idle":"2021-11-27T15:32:43.847964Z","shell.execute_reply.started":"2021-11-27T15:32:43.824079Z","shell.execute_reply":"2021-11-27T15:32:43.847257Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"data.label.value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-11-27T15:37:07.037093Z","iopub.execute_input":"2021-11-27T15:37:07.038110Z","iopub.status.idle":"2021-11-27T15:37:07.049918Z","shell.execute_reply.started":"2021-11-27T15:37:07.038054Z","shell.execute_reply":"2021-11-27T15:37:07.048996Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"#for convert words to numerical data by mapping \nlabel_mapping={'stressed':1,'not-stressed':2}\n\ndata['label']=data['label'].map(label_mapping)\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-27T15:37:14.177022Z","iopub.execute_input":"2021-11-27T15:37:14.177319Z","iopub.status.idle":"2021-11-27T15:37:14.197008Z","shell.execute_reply.started":"2021-11-27T15:37:14.177287Z","shell.execute_reply":"2021-11-27T15:37:14.196041Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"data.duplicated().sum()   #to know how many duplicated ","metadata":{"execution":{"iopub.status.busy":"2021-11-27T15:37:30.065327Z","iopub.execute_input":"2021-11-27T15:37:30.066198Z","iopub.status.idle":"2021-11-27T15:37:30.099696Z","shell.execute_reply.started":"2021-11-27T15:37:30.066158Z","shell.execute_reply":"2021-11-27T15:37:30.099061Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"data=data.drop_duplicates()   #to delete duplicate\ndata = data.reset_index(drop=True)\n","metadata":{"execution":{"iopub.status.busy":"2021-11-27T15:37:35.170186Z","iopub.execute_input":"2021-11-27T15:37:35.170800Z","iopub.status.idle":"2021-11-27T15:37:35.206484Z","shell.execute_reply.started":"2021-11-27T15:37:35.170751Z","shell.execute_reply":"2021-11-27T15:37:35.205384Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"data.shape[0]","metadata":{"execution":{"iopub.status.busy":"2021-11-27T15:37:38.009991Z","iopub.execute_input":"2021-11-27T15:37:38.010297Z","iopub.status.idle":"2021-11-27T15:37:38.016278Z","shell.execute_reply.started":"2021-11-27T15:37:38.010270Z","shell.execute_reply":"2021-11-27T15:37:38.015320Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"markdown","source":"Text PreprocessingÂ¶\n","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm\nimport re\nfrom bs4 import BeautifulSoup\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\n\ndef decontracted(phrase):\n    \"\"\"\n    We first define a function to expand the contracted phrase into normal words\n    \"\"\"\n    # specific\n    phrase = re.sub(r\"wont\", \"will not\", phrase)\n    phrase = re.sub(r\"wouldnt\", \"would not\", phrase)\n    phrase = re.sub(r\"shouldnt\", \"should not\", phrase)\n    phrase = re.sub(r\"couldnt\", \"could not\", phrase)\n    phrase = re.sub(r\"cudnt\", \"could not\", phrase)\n    phrase = re.sub(r\"cant\", \"can not\", phrase)\n    phrase = re.sub(r\"dont\", \"do not\", phrase)\n    phrase = re.sub(r\"doesnt\", \"does not\", phrase)\n    phrase = re.sub(r\"didnt\", \"did not\", phrase)\n    phrase = re.sub(r\"wasnt\", \"was not\", phrase)\n    phrase = re.sub(r\"werent\", \"were not\", phrase)\n    phrase = re.sub(r\"havent\", \"have not\", phrase)\n    phrase = re.sub(r\"hadnt\", \"had not\", phrase)\n\n    # general\n    phrase = re.sub(r\"n\\ t\", \" not\", phrase)\n    #phrase = re.sub(r\"\\re\", \" are\", phrase)\n    phrase = re.sub(r\"\\ s \", \" is \", phrase) # prime \n    phrase = re.sub(r\"\\ d \", \" would \", phrase)\n    phrase = re.sub(r\"\\ ll \", \" will \", phrase)\n    phrase = re.sub(r\"\\dunno\", \"do not \", phrase)\n    phrase = re.sub(r\"ive \", \"i have \", phrase)\n    phrase = re.sub(r\"im \", \"i am \", phrase)\n    phrase = re.sub(r\"i m \", \"i am \", phrase)\n    phrase = re.sub(r\" w \", \" with \", phrase)\n    \n    return phrase\n    \ndef clean_text(data):\n    \"\"\"\n    Clean the review texts\n    \"\"\"\n    cleaned_review = []\n\n    for review_text in tqdm(data['sentence']):\n        \n        # expand the contracted words\n        review_text = decontracted(review_text)\n        #remove html tags\n        review_text = BeautifulSoup(review_text, 'lxml').get_text().strip() # re.sub(r'<.*?>', '', text)\n        \n        #remove non-alphabetic characters\n        review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n    \n        #remove url \n        review_text = re.sub(r'https?://\\S+|www\\.\\S+', '', review_text)\n        \n        #Removing punctutation, string.punctuation in python consists of !\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_{|}~`\n       # review_text = review_text.translate(str.maketrans('', '', string.punctuation))\n        # ''.join([char for char in movie_text_data if char not in string.punctuation])\n        \n        # remove emails\n        review_text = re.sub(r\"(^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+$)\", '', review_text)\n    \n        cleaned_review.append(review_text)\n\n    return cleaned_review  \n\ndata['cleaned_sentence'] = clean_text(data)\ndata.head() ","metadata":{"execution":{"iopub.status.busy":"2021-11-27T15:37:43.721913Z","iopub.execute_input":"2021-11-27T15:37:43.722252Z","iopub.status.idle":"2021-11-27T15:37:49.553965Z","shell.execute_reply.started":"2021-11-27T15:37:43.722208Z","shell.execute_reply":"2021-11-27T15:37:49.552970Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"#lemmmentation\nfrom nltk.tokenize import RegexpTokenizer\nimport nltk\nnltk.download('punkt')\ndef remove_stopwords(phrase):\n    remove_sw = []\n    tokenizer = RegexpTokenizer(r'[a-zA-Z0-9]+')\n    stop_words = stopwords.words('english')\n    \n    for review_text in tqdm(phrase):\n        tokens = word_tokenize(review_text)\n        tokens = [word for word in tokens if not word in stop_words]\n        remove_sw.append(tokens)\n    return remove_sw\n\ndata['cleaned_sentence'] = remove_stopwords(data['cleaned_sentence'])\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-27T15:37:49.555749Z","iopub.execute_input":"2021-11-27T15:37:49.556026Z","iopub.status.idle":"2021-11-27T15:38:14.963976Z","shell.execute_reply.started":"2021-11-27T15:37:49.555995Z","shell.execute_reply":"2021-11-27T15:38:14.963206Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"#stemming for extract the actual meaning of the words\nfrom nltk.stem import PorterStemmer\n\ndef stemming(phrase):\n    stemmer = PorterStemmer()\n    stem_output=[]\n    stemmed=[]\n    for review_text in tqdm(phrase):\n        stemmed = [stemmer.stem(word) for word in review_text]\n        stem_output.append(stemmed)\n    return stem_output\n\ndata['cleaned_sentence'] = stemming(data['cleaned_sentence'])\ndata['cleaned_sentence'].head()","metadata":{"execution":{"iopub.status.busy":"2021-11-27T15:38:14.965273Z","iopub.execute_input":"2021-11-27T15:38:14.965473Z","iopub.status.idle":"2021-11-27T15:38:21.399793Z","shell.execute_reply.started":"2021-11-27T15:38:14.965447Z","shell.execute_reply":"2021-11-27T15:38:21.399001Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"def to_sentence(phrase):\n    sentence=[]\n    for words in tqdm(phrase):\n        sentence.append((\" \").join(words))\n    return sentence\ndata['cleaned_sentence']=to_sentence(data['cleaned_sentence'])\ndata['cleaned_sentence'].head()","metadata":{"execution":{"iopub.status.busy":"2021-11-27T15:38:21.401401Z","iopub.execute_input":"2021-11-27T15:38:21.401612Z","iopub.status.idle":"2021-11-27T15:38:21.455911Z","shell.execute_reply.started":"2021-11-27T15:38:21.401587Z","shell.execute_reply":"2021-11-27T15:38:21.454991Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"# convert the cleaned sentences to vectors\ntoken = RegexpTokenizer(r'[a-zA-Z0-9]+')\nvectorizer = CountVectorizer(stop_words='english', max_df=0.5, min_df=3, ngram_range=(1,1),tokenizer = token.tokenize)\nx = vectorizer.fit_transform(data.cleaned_sentence)\ny = data.label.values\n\nprint(\"X.shape : \",x.shape)\nprint(\"y.shape : \",y.shape)\n","metadata":{"execution":{"iopub.status.busy":"2021-11-27T15:38:29.432926Z","iopub.execute_input":"2021-11-27T15:38:29.433273Z","iopub.status.idle":"2021-11-27T15:38:29.760743Z","shell.execute_reply.started":"2021-11-27T15:38:29.433238Z","shell.execute_reply":"2021-11-27T15:38:29.759782Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"markdown","source":"split data \"train+validation+test\" to train and test","metadata":{"execution":{"iopub.status.busy":"2021-11-27T03:37:58.420123Z","iopub.execute_input":"2021-11-27T03:37:58.420423Z","iopub.status.idle":"2021-11-27T03:37:58.426202Z","shell.execute_reply.started":"2021-11-27T03:37:58.420392Z","shell.execute_reply":"2021-11-27T03:37:58.425164Z"}}},{"cell_type":"code","source":"\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.20,shuffle=True, random_state=42)\nx_train","metadata":{"execution":{"iopub.status.busy":"2021-11-27T15:38:41.569157Z","iopub.execute_input":"2021-11-27T15:38:41.569458Z","iopub.status.idle":"2021-11-27T15:38:41.579893Z","shell.execute_reply.started":"2021-11-27T15:38:41.569424Z","shell.execute_reply":"2021-11-27T15:38:41.579068Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"x_train.shape","metadata":{"execution":{"iopub.status.busy":"2021-11-27T15:34:21.034569Z","iopub.execute_input":"2021-11-27T15:34:21.034845Z","iopub.status.idle":"2021-11-27T15:34:21.040934Z","shell.execute_reply.started":"2021-11-27T15:34:21.034817Z","shell.execute_reply":"2021-11-27T15:34:21.040136Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"# fit a logistic regression classifier on the training data use default settings\nlr_clf = LogisticRegression(solver='liblinear')\nlr_clf.fit(x_train, y_train)\n\n# make prediction on testing data\ny_pred_test_lr = lr_clf.predict(x_test)\ny_predprob_lr = lr_clf.predict_proba(x_test)\nmatrix_lr = confusion_matrix(y_test,y_pred_test_lr)\nprint(classification_report(y_test, y_pred_test_lr))\nprint(\"\\nAccuracy for Logistic Regression model:\",metrics.accuracy_score(y_test, y_pred_test_lr))\nprint(\"\\n\")\ny_predict = lr_clf.predict(x_test)\nmatrix_display = ConfusionMatrixDisplay(matrix_lr).plot()","metadata":{"execution":{"iopub.status.busy":"2021-11-27T15:38:51.708498Z","iopub.execute_input":"2021-11-27T15:38:51.708788Z","iopub.status.idle":"2021-11-27T15:38:52.025332Z","shell.execute_reply.started":"2021-11-27T15:38:51.708756Z","shell.execute_reply":"2021-11-27T15:38:52.024633Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"from sklearn.neural_network import MLPClassifier\nmlp = MLPClassifier(hidden_layer_sizes=(10, 10, 10), max_iter=1000)\nmlp.fit(x_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2021-11-27T15:45:49.215012Z","iopub.execute_input":"2021-11-27T15:45:49.215332Z","iopub.status.idle":"2021-11-27T15:45:58.486144Z","shell.execute_reply.started":"2021-11-27T15:45:49.215298Z","shell.execute_reply":"2021-11-27T15:45:58.485344Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"predictions = mlp.predict(x_test)","metadata":{"execution":{"iopub.status.busy":"2021-11-27T15:46:31.110067Z","iopub.execute_input":"2021-11-27T15:46:31.110361Z","iopub.status.idle":"2021-11-27T15:46:31.120017Z","shell.execute_reply.started":"2021-11-27T15:46:31.110329Z","shell.execute_reply":"2021-11-27T15:46:31.118996Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix\nprint(confusion_matrix(y_test,predictions))\nprint(classification_report(y_test,predictions))","metadata":{"execution":{"iopub.status.busy":"2021-11-27T15:46:51.984857Z","iopub.execute_input":"2021-11-27T15:46:51.985229Z","iopub.status.idle":"2021-11-27T15:46:52.011771Z","shell.execute_reply.started":"2021-11-27T15:46:51.985184Z","shell.execute_reply":"2021-11-27T15:46:52.010689Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}